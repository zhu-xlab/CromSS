
import torch
import numpy as np
import matplotlib.pyplot as plt


# # # # remove prefix generated by hpc # # # # 
def modify_module_state_dict_keys(msd):
    prefix = 'module.'
    for k in list(msd.keys()):
        if k.startswith(prefix):
            newk = k[len(prefix):]
            msd[newk] = msd[k]
            del msd[k]
    return msd


# load the entire encoder 
def load_pretrain_encoder(net, ckp_path, device=torch.device('cpu'), pretrain_type='ns'):
    # load weights
    if pretrain_type=='dino':
        state_dict = torch.load(ckp_path, map_location=device)['teacher']
        
        # remove the prefix of dino model
        pf_sdict='module.backbone.'
        for k in list(state_dict.keys()):
            if k.startswith(pf_sdict):
                newk = 'encoder.'+k[len(pf_sdict):]
                state_dict[newk] = state_dict[k]
            del state_dict[k]
    elif pretrain_type=='ns':
        state_dict = torch.load(ckp_path, map_location=device)
        state_dict = modify_module_state_dict_keys(state_dict['model_state_dict'])
    
    # delete decoder keys from state_dict
    del_keys = []
    for key in state_dict:
        if key[:2] in ['de', 'se']:
            del_keys.append(key)
    for key in del_keys:
        state_dict.pop(key)
    
    #load weights
    net.load_state_dict(state_dict,strict=False)
    
    return


# load weights for the entire network
def load_pretrain_model(net, ckp_path, device=torch.device('cpu'), fix=True):
    # load weights
    state_dict = torch.load(ckp_path, map_location=device)
    state_dict = modify_module_state_dict_keys(state_dict['model_state_dict'])
    
    # load weights
    net.load_state_dict(state_dict,strict=True)
    
    # fix model weights
    if fix:
        for name, param in net.named_parameters():
            param.requires_grad = False
    
    return


# fix encoder
def fix_encoder(net):
    for name, param in net.named_parameters():
        if 'encoder' in name:
            param.requires_grad = False
    return


# unfix encoder
def unfix_encoder(net):
    for name, param in net.named_parameters():
        if 'encoder' in name:
            param.requires_grad = True
    return


# load specific layers
def load_pretrain_layers(net, ckp_path, fine_tune, device, layers='down4', dino_type=None):
    # load weights
    if dino_type:
        assert(dino_type == 'teacher' or dino_type == 'student')
        state_dict = torch.load(ckp_path, map_location=device)[dino_type]
        print('Dino-{} model loaded!\n'.format(dino_type))
        
        # remove the prefix of dino model
        pf_sdict='module.backbone.'
        for k in list(state_dict.keys()):
            if k.startswith(pf_sdict):
                newk = k[len(pf_sdict):]
                state_dict[newk] = state_dict[k]
                #print(k, '', 'module.encoder.' + k[len("module.backbone."):])
            del state_dict[k]
    else:
        state_dict = torch.load(ckp_path, map_location=device)
        print('\n')
    
    # prefixes for loaded/fixed layers
    # inc
    lnames = []
    lnames.append('inc')
    # down
    if not layers.startswith('inc'):
        if layers.startswith('down'):
            dn = int(layers[-1])
            un = 0
        else:
            dn = 4
            un = 4
        for i in range(1,dn+1):
            lnames.append(f'down{i}')
        # up
        if un:
            assert(layers.startswith('up'))
            un = int(layers[-1])
            for i in range(1,un+1):
                lnames.append(f'up{i}')

    # fix pretrained layers
    if not fine_tune:
        for name, param in net.named_parameters():
            if (name[:3] in lnames) or (name[:5] in lnames):
                param.requires_grad = False
    
    # delete decoder keys from state_dict
    del_keys = []
    for key in state_dict:
        if (key[:3] not in lnames) and (key[:5] not in lnames):
            del_keys.append(key)
    
    for key in del_keys:
        state_dict.pop(key)
    
    #load weights
    net.load_state_dict(state_dict,strict=False)
    
    return net


# for ResNet18UNet
def load_dino_resnet_encoder(net, ckp_path, dino_type, fine_tune, device, 
                             pf_net='base_model', pf_sdict='module.backbone'):
    # prefix of net: 'base_model' for ResNet18UNet; 'encoder' for smp.Unet
    if dino_type:
        assert(dino_type == 'teacher' or dino_type == 'student')
        state_dict = torch.load(ckp_path, map_location=device)[dino_type]
        print('Dino-{} model loaded!\n'.format(dino_type))
    else:
        state_dict = torch.load(ckp_path, map_location=device)
        print('\n')
    
    # fix pretrained layers
    if not fine_tune:
        for name, param in net.named_parameters():
            if name.startswith(pf_net):
                param.requires_grad = False
    
    # modify keys in state_dict
    for k in list(state_dict.keys()):
        if k.startswith(pf_sdict):
            newk = pf_net+k[len(pf_sdict):]
            state_dict[newk] = state_dict[k]
            #print(k, '', 'module.encoder.' + k[len("module.backbone."):])
        del state_dict[k]

    #load weights
    msg = net.load_state_dict(state_dict,strict=False)
    for k in msg.missing_keys:
        assert not k.startswith(pf_net)
    
    return net


def plot_img_and_mask(img, mask):
    classes = mask.shape[0] if len(mask.shape) > 2 else 1
    fig, ax = plt.subplots(1, classes + 1)
    ax[0].set_title('Input image')
    ax[0].imshow(img)
    if classes > 1:
        for i in range(classes):
            ax[i + 1].set_title(f'Output mask (class {i + 1})')
            ax[i + 1].imshow(mask[:, :, i])
    else:
        ax[1].set_title('Output mask')
        ax[1].imshow(mask)
    plt.xticks([]), plt.yticks([])
    plt.show()


# set adaptive lrs for different layers
BLOCK_NAMES = ['encoder.layer1', 'encoder.layer2', 'encoder.layer3', 'encoder.layer4',
               'decoder.blocks.0', 'decoder.blocks.1', 'decoder.blocks.2', 'decoder.blocks.3', 'decoder.blocks.4',
               'segmentation_head']

def set_adaptive_lr_for_layers(net, lr_base, lr_adaptive_type='des', lr_adaptive_rate=0.8):
    assert lr_adaptive_type in ['des', 'asc'], "Please provide correct lr adaptive type ('asc' or 'des')!"
    params = []
    lr = lr_base
    
    block_names = BLOCK_NAMES
    
    # get lr list for different layers
    nbs = len(block_names)
    lrs = [lr*lr_adaptive_rate**i for i in range(nbs+1)]
    if lr_adaptive_type == 'asc':
        lrs.reverse()
    
    # set lr
    for name, param in net.named_parameters():
        sign = True
        for bi, block_name in enumerate(block_names):
            if block_name in name:
                params.append({'params': param, 'lr': lrs[bi+1]})
                sign = False
                break
        if sign:
            params.append({'params': param, 'lr': lrs[0]})
    
    return params
                
        
def set_adaptive_lr_for_layers_decoder(net, lr_base, lr_adaptive_type='des', lr_adaptive_rate=0.8):
    assert lr_adaptive_type in ['des', 'asc'], "Please provide correct lr adaptive type ('asc' or 'des')!"
    params = []
    lr = lr_base
    
    block_names = BLOCK_NAMES[-6:]
    
    # get lr list for different layers
    nbs = len(block_names)
    lrs = [lr*lr_adaptive_rate**i for i in range(nbs)]
    if lr_adaptive_type == 'asc':
        lrs.reverse()
    
    # set lr
    for name, param in net.named_parameters():
        for bi, block_name in enumerate(block_names):
            if block_name in name:
                params.append({'params': param, 'lr': lrs[bi]})
                break
        
    return params

# # # # # # # # # # # #   ramp up   # # # # # # # # # # # #
def sigmoid_rampup(current, rampup_length):
    """Exponential rampup from https://arxiv.org/abs/1610.02242"""
    if rampup_length == 0:
        return 1.0
    else:
        current = np.clip(current, 0.0, rampup_length)
        phase = 1.0 - current / rampup_length
        return float(np.exp(-5.0 * phase * phase))


def linear_rampup(current, rampup_length):
    """Linear rampup"""
    assert current >= 0 and rampup_length >= 0
    if current >= rampup_length:
        return 1.0
    else:
        return current / rampup_length


def cosine_rampdown(current, rampdown_length):
    """Cosine rampdown from https://arxiv.org/abs/1608.03983"""
    assert 0 <= current <= rampdown_length
    return float(.5 * (np.cos(np.pi * current / rampdown_length) + 1))












